{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e43131f",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "**Project Introduction**\n",
    "\n",
    "Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows you to modify foundation models for your needs without needing substantial computational resources.\n",
    "\n",
    "In this project, you will apply parameter-efficient fine-tuning using the Hugging Face `peft` library.\n",
    "\n",
    "**Project Summary**\n",
    "\n",
    "In this project, you will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, you will:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb974d1",
   "metadata": {},
   "source": [
    "## HuggingFace PEFT Library\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "Hugging Face PEFT allows you to fine-tune a model without having to fine-tune all of its parameters.\n",
    "\n",
    "Training a model using Hugging Face PEFT requires two additional steps beyond traditional fine-tuning:\n",
    "\n",
    "1. Creating a **PEFT config**\n",
    "2. **Converting the model into a PEFT model** using the PEFT config\n",
    "Inference using a PEFT model is almost identical to inference using a non-PEFT model. The only difference is that it must be loaded as a PEFT model.\n",
    "\n",
    "## Training with PEFT\n",
    "\n",
    "Creating a PEFT Config\n",
    "The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a `PeftConfig`, but this example will use a `LoraConfig`, the subclass used for low rank adaptation (LoRA).\n",
    "\n",
    "A LoRA config can be instantiated like this:\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "config = LoraConfig()\n",
    "```\n",
    "\n",
    "Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to `LoraConfig()`. [Hugging Face LoRA conceptual guide](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) also contains additional explanations.\n",
    "\n",
    "## Converting a Transformers Model into a PEFT Model\n",
    "\n",
    "Once you have a PEFT config object, you can load a Hugging Face `transformers` model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "```\n",
    "\n",
    "Then using `get_peft_model()` to get a trainable PEFT model (using the LoRA config instantiated previously):\n",
    "\n",
    "```python\n",
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "## Training with a PEFT Model\n",
    "\n",
    "After calling `get_peft_model()`, you can then use the resulting `lora_model` in a training process of your choice (PyTorch training loop or Hugging Face `Trainer`).\n",
    "\n",
    "## Checking Trainable Parameters of a PEFT Model\n",
    "\n",
    "A helpful way to check the number of trainable parameters with the current config is the `print_trainable_parameters()` method:\n",
    "\n",
    "```python\n",
    "lora_model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "Which prints an output like this:\n",
    "\n",
    "`trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364`\n",
    "\n",
    "## Saving a Trained PEFT Model\n",
    "\n",
    "Once a PEFT model has been trained, the standard Hugging Face `save_pretrained()` method can be used to save the weights locally. For example:\n",
    "\n",
    "```python\n",
    "lora_model.save_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "Note that this **only saves the adapter weights** and not the weights of the original Transformers model. Thus the size of the files created will be much smaller than you might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42be92",
   "metadata": {},
   "source": [
    "# Inference with PEFT\n",
    "\n",
    "## Loading a Saved PEFT Model\n",
    "\n",
    "Because you have only saved the adapter weights and not the full model weights, you can't use `from_pretrained()` with the regular Transformers class (e.g., `AutoModelForCausalLM`). Instead, you need to use the PEFT version (e.g., `AutoPeftModelForCausalLM`). For example:\n",
    "\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "lora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "After completing this step, you can proceed to use the model for inference.\n",
    "\n",
    "# Generating Text from a PEFT Model\n",
    "\n",
    "You may see examples from regular Transformer models where the input IDs are passed in as a positional argument (e.g., `model.generate(input_ids)`). For a PEFT model, they must be passed in as a keyword argument (e.g., `model.generate(input_ids=input_ids)`). For example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "```\n",
    "\n",
    "**Documentation Links**\n",
    "\n",
    "* [Hugging Face PEFT configuration](https://huggingface.co/docs/peft/package_reference/config)\n",
    "* [Hugging Face LoRA adapter](https://huggingface.co/docs/peft/package_reference/lora)\n",
    "* [Hugging Face Models save_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)\n",
    "* [Hugging Face Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347a791",
   "metadata": {},
   "source": [
    "# Project Instructions\n",
    "\n",
    "To pass this project, your code must:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine-tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This project is failry open-ended. As long as you follow the prescribed steps **you may choose any appropriate PEFT technique, model, evaluation approach, and fine-tuning dataset.**\n",
    "\n",
    "* **PEFT Technique**:\n",
    "\n",
    "> * The PEFT technique covered in this course was LoRA, but new techniques are continuously being developed. See the [PEFT README](https://github.com/huggingface/peft) for links to the papers behind each of the supported techniques.\n",
    "> * If you are unsure, we recommend using **LoRA** as your PEFT technique. LoRA is the only PEFT technique that is compatible with all models at this time.\n",
    "\n",
    "* **Model**\n",
    "\n",
    "> * Your choice of model will depend on your choice of PEFT technique.\n",
    "> * Unless you plan to use your own hardware/GPU rather than the Udacity Workspace, it's best to choose a smaller model.\n",
    "> * The model must be compatible with a sequence classification task.\n",
    "> * If you are unsure, we recommend using GPT-2 as your model. This is a relatively small model that is compatible with sequence classification and LoRA.\n",
    "\n",
    "For specific model names in the Hugging Face registry, you can use the widget at the bottom of the [PEFT documentation homepage](https://huggingface.co/docs/peft/index) (select \"sequence classification\" from the drop-down).\n",
    "\n",
    "* **Evaluation Approach**\n",
    "\n",
    "> * The evaluation approach covered in this course was the `evaluate` method with a Hugging Face `Trainer`. You may use the same approach, or any other reasonable evaluation approach for a sequence classification task\n",
    "> * The key requirement for the evaluation is that you must be able to compare the original foundation model's performance and the fine-tuned model's performance.\n",
    "\n",
    "* **Dataset**\n",
    "\n",
    "> * Your PEFT process must use a dataset from Hugging Face's `datasets` library. As with the selection of model, you will need to ensure that the dataset is small enough that it is usable for your workspace\n",
    "> * The key requirement for the dataset is that it matches the task. Follow this link to [view Hugging Face datasets filtered by the text classification task](https://huggingface.co/datasets?task_categories=task_categories:text-classification)\n",
    "\n",
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "### Loading the model\n",
    "\n",
    "Once you have selected a model, load it in your notebook.\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "Perform an initial evaluation of the model on your chosen sequence classification task. This step will require that you also load an appropriate tokenizer and dataset.\n",
    "\n",
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "### Creating a PEFT config\n",
    "\n",
    "Create a PEFT config with appropriate hyperparameters for your chosen model.\n",
    "\n",
    "### Creating a PEFT model\n",
    "\n",
    "Using the PEFT config and foundation model, create a PEFT model.\n",
    "\n",
    "### Training the model\n",
    "\n",
    "Using the PEFT model and dataset, run a training loop with at least one epoch.\n",
    "\n",
    "### Saving the trained model\n",
    "\n",
    "Depending on your training loop configuration, your PEFT model may have already been saved. If not, use `save_pretrained` to save your progress\n",
    "\n",
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Using the appropriate PEFT model class, load your trained model.\n",
    "\n",
    "## Evaluating the model\n",
    "\n",
    "Repeat the previous evaluation process, this time using the PEFT model. Compare the results to the results from the original foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa4fda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5c51087",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
