{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e43131f",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "**Project Introduction**\n",
    "\n",
    "Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows you to modify foundation models for your needs without needing substantial computational resources.\n",
    "\n",
    "In this project, you will apply parameter-efficient fine-tuning using the Hugging Face `peft` library.\n",
    "\n",
    "**Project Summary**\n",
    "\n",
    "In this project, you will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, you will:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb974d1",
   "metadata": {},
   "source": [
    "## HuggingFace PEFT Library\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "Hugging Face PEFT allows you to fine-tune a model without having to fine-tune all of its parameters.\n",
    "\n",
    "Training a model using Hugging Face PEFT requires two additional steps beyond traditional fine-tuning:\n",
    "\n",
    "1. Creating a **PEFT config**\n",
    "2. **Converting the model into a PEFT model** using the PEFT config\n",
    "Inference using a PEFT model is almost identical to inference using a non-PEFT model. The only difference is that it must be loaded as a PEFT model.\n",
    "\n",
    "## Training with PEFT\n",
    "\n",
    "Creating a PEFT Config\n",
    "The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a `PeftConfig`, but this example will use a `LoraConfig`, the subclass used for low rank adaptation (LoRA).\n",
    "\n",
    "A LoRA config can be instantiated like this:\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "config = LoraConfig()\n",
    "```\n",
    "\n",
    "Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to `LoraConfig()`. [Hugging Face LoRA conceptual guide](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) also contains additional explanations.\n",
    "\n",
    "## Converting a Transformers Model into a PEFT Model\n",
    "\n",
    "Once you have a PEFT config object, you can load a Hugging Face `transformers` model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "```\n",
    "\n",
    "Then using `get_peft_model()` to get a trainable PEFT model (using the LoRA config instantiated previously):\n",
    "\n",
    "```python\n",
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "## Training with a PEFT Model\n",
    "\n",
    "After calling `get_peft_model()`, you can then use the resulting `lora_model` in a training process of your choice (PyTorch training loop or Hugging Face `Trainer`).\n",
    "\n",
    "## Checking Trainable Parameters of a PEFT Model\n",
    "\n",
    "A helpful way to check the number of trainable parameters with the current config is the `print_trainable_parameters()` method:\n",
    "\n",
    "```python\n",
    "lora_model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "Which prints an output like this:\n",
    "\n",
    "`trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364`\n",
    "\n",
    "## Saving a Trained PEFT Model\n",
    "\n",
    "Once a PEFT model has been trained, the standard Hugging Face `save_pretrained()` method can be used to save the weights locally. For example:\n",
    "\n",
    "```python\n",
    "lora_model.save_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "Note that this **only saves the adapter weights** and not the weights of the original Transformers model. Thus the size of the files created will be much smaller than you might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42be92",
   "metadata": {},
   "source": [
    "# Inference with PEFT\n",
    "\n",
    "## Loading a Saved PEFT Model\n",
    "\n",
    "Because you have only saved the adapter weights and not the full model weights, you can't use `from_pretrained()` with the regular Transformers class (e.g., `AutoModelForCausalLM`). Instead, you need to use the PEFT version (e.g., `AutoPeftModelForCausalLM`). For example:\n",
    "\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "lora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "After completing this step, you can proceed to use the model for inference.\n",
    "\n",
    "# Generating Text from a PEFT Model\n",
    "\n",
    "You may see examples from regular Transformer models where the input IDs are passed in as a positional argument (e.g., `model.generate(input_ids)`). For a PEFT model, they must be passed in as a keyword argument (e.g., `model.generate(input_ids=input_ids)`). For example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "```\n",
    "\n",
    "**Documentation Links**\n",
    "\n",
    "* [Hugging Face PEFT configuration](https://huggingface.co/docs/peft/package_reference/config)\n",
    "* [Hugging Face LoRA adapter](https://huggingface.co/docs/peft/package_reference/lora)\n",
    "* [Hugging Face Models save_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)\n",
    "* [Hugging Face Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347a791",
   "metadata": {},
   "source": [
    "# Project Instructions\n",
    "\n",
    "To pass this project, your code must:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine-tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This project is failry open-ended. As long as you follow the prescribed steps **you may choose any appropriate PEFT technique, model, evaluation approach, and fine-tuning dataset.**\n",
    "\n",
    "* **PEFT Technique**:\n",
    "\n",
    "> * The PEFT technique covered in this course was LoRA, but new techniques are continuously being developed. See the [PEFT README](https://github.com/huggingface/peft) for links to the papers behind each of the supported techniques.\n",
    "> * If you are unsure, we recommend using **LoRA** as your PEFT technique. LoRA is the only PEFT technique that is compatible with all models at this time.\n",
    "\n",
    "* **Model**\n",
    "\n",
    "> * Your choice of model will depend on your choice of PEFT technique.\n",
    "> * Unless you plan to use your own hardware/GPU rather than the Udacity Workspace, it's best to choose a smaller model.\n",
    "> * The model must be compatible with a sequence classification task.\n",
    "> * If you are unsure, we recommend using GPT-2 as your model. This is a relatively small model that is compatible with sequence classification and LoRA.\n",
    "\n",
    "For specific model names in the Hugging Face registry, you can use the widget at the bottom of the [PEFT documentation homepage](https://huggingface.co/docs/peft/index) (select \"sequence classification\" from the drop-down).\n",
    "\n",
    "* **Evaluation Approach**\n",
    "\n",
    "> * The evaluation approach covered in this course was the `evaluate` method with a Hugging Face `Trainer`. You may use the same approach, or any other reasonable evaluation approach for a sequence classification task\n",
    "> * The key requirement for the evaluation is that you must be able to compare the original foundation model's performance and the fine-tuned model's performance.\n",
    "\n",
    "* **Dataset**\n",
    "\n",
    "> * Your PEFT process must use a dataset from Hugging Face's `datasets` library. As with the selection of model, you will need to ensure that the dataset is small enough that it is usable for your workspace\n",
    "> * The key requirement for the dataset is that it matches the task. Follow this link to [view Hugging Face datasets filtered by the text classification task](https://huggingface.co/datasets?task_categories=task_categories:text-classification)\n",
    "\n",
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "### Loading the model\n",
    "\n",
    "Once you have selected a model, load it in your notebook.\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "Perform an initial evaluation of the model on your chosen sequence classification task. This step will require that you also load an appropriate tokenizer and dataset.\n",
    "\n",
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "### Creating a PEFT config\n",
    "\n",
    "Create a PEFT config with appropriate hyperparameters for your chosen model.\n",
    "\n",
    "### Creating a PEFT model\n",
    "\n",
    "Using the PEFT config and foundation model, create a PEFT model.\n",
    "\n",
    "### Training the model\n",
    "\n",
    "Using the PEFT model and dataset, run a training loop with at least one epoch.\n",
    "\n",
    "### Saving the trained model\n",
    "\n",
    "Depending on your training loop configuration, your PEFT model may have already been saved. If not, use `save_pretrained` to save your progress\n",
    "\n",
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Using the appropriate PEFT model class, load your trained model.\n",
    "\n",
    "## Evaluating the model\n",
    "\n",
    "Repeat the previous evaluation process, this time using the PEFT model. Compare the results to the results from the original foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa4fda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5c51087",
   "metadata": {},
   "source": [
    "# Actual Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6338eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, get_peft_model\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM, DataCollatorWithPadding, Trainer, TrainingArguments, AutoTokenizer)\n",
    "# from datasets import load_dataset\n",
    "# import evaluate\n",
    "# import torch\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1230b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Claude Help\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2ForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4213e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found!\n"
     ]
    }
   ],
   "source": [
    "# Set up compute device\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found!\")\n",
    "else:\n",
    "    print(\"MPS device not found, using CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set default device to MPS (optional)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9719ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/mteb/InsurancePolicyInterpretationLegalBenchClassification\n",
    "# https://huggingface.co/datasets/Yelp/yelp_review_full\n",
    "# Bring in the data set\n",
    "ds_ins = load_dataset(\"mteb/InsurancePolicyInterpretationLegalBenchClassification\")\n",
    "ds_yelp = load_dataset(\"Yelp/yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cce197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2df7280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247953e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset - Train: 10000, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "# Reduce size of ds_yelp\n",
    "SUBSET_SIZE = 10000  # Set to None to use full dataset\n",
    "if SUBSET_SIZE:\n",
    "    ds_yelp[\"train\"] = ds_yelp[\"train\"].select(range(min(SUBSET_SIZE, len(ds_yelp[\"train\"]))))\n",
    "    ds_yelp[\"test\"] = ds_yelp[\"test\"].select(range(min(SUBSET_SIZE//5, len(ds_yelp[\"test\"]))))\n",
    "    print(f\"Using subset - Train: {len(ds_yelp['train'])}, Test: {len(ds_yelp['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a577e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "Columns: ['label', 'text']\n",
      "Data types:\n",
      "label     int64\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "Label Distribution (Train):\n",
      "  1 stars: 1788 reviews (17.9%)\n",
      "  2 stars: 2168 reviews (21.7%)\n",
      "  3 stars: 2361 reviews (23.6%)\n",
      "  4 stars: 2082 reviews (20.8%)\n",
      "  5 stars: 1601 reviews (16.0%)\n",
      "\n",
      "Sample Reviews:\n",
      "\n",
      "Review 1 (5 stars):\n",
      "Text: dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-no...\n",
      "\n",
      "Review 2 (2 stars):\n",
      "Text: Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply ...\n",
      "\n",
      "Review 3 (4 stars):\n",
      "Text: Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of hi...\n",
      "\n",
      "Text Statistics:\n",
      "Average words per review: 130.2\n",
      "Median words per review: 99.0\n",
      "Max words: 937\n",
      "Min words: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the data\n",
    "train_df = ds_yelp[\"train\"].to_pandas()\n",
    "test_df = ds_yelp[\"test\"].to_pandas()\n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{train_df.dtypes}\")\n",
    "\n",
    "print(\"\\nLabel Distribution (Train):\")\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label + 1} stars: {count} reviews ({count/len(train_df)*100:.1f}%)\")\n",
    "    # Introduce bar plot from seaborn\n",
    "train_df[\"label\"].plot(kind=\"hist\", title=\"Label Distribution Plot\")\n",
    "\n",
    "print(\"\\nSample Reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nReview {i+1} ({train_df.iloc[i]['label'] + 1} stars):\")\n",
    "    print(f\"Text: {train_df.iloc[i]['text'][:200]}...\")\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "text_lengths = [len(text.split()) for text in train_df['text']]\n",
    "print(f\"Average words per review: {np.mean(text_lengths):.1f}\")\n",
    "print(f\"Median words per review: {np.median(text_lengths):.1f}\")\n",
    "print(f\"Max words: {np.max(text_lengths)}\")\n",
    "print(f\"Min words: {np.min(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64569968",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "### Loading the model\n",
    "\n",
    "Once you have selected a model, load it in your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9518f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# Load tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11027b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: gpt2\n",
      "Vocab size: 50257\n",
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Add padding token (GPT-2 doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d17bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for sequence classification (5 classes for 1-5 star ratings, pad-token\n",
    "num_label = len(train_df[\"label\"].unique())\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_label,\n",
    "    pad_token_id=tokenizer.pad_token\n",
    ")\n",
    "# Fix the pad_token_id configuration issue\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Resize token embeddings to match tokenizer (in case pad token was added)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2fd68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to mps\n",
      "Model parameters: 124,443,648\n",
      "Pad token ID: 50256\n",
      "EOS token ID: 50256\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(f\"Model loaded and moved to {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Pad token ID: {model.config.pad_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b266be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data usable for gpt2\n",
    "max_length = 512  # Maximum sequence length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the text and prepare labels\"\"\"\n",
    "    # Tokenize the text - don't use return_tensors for batch processing\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Labels are already 0-4, which matches our model's expected format\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b07c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8aa999c4104f0893b725f9900a7fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6563a05cf343d3b6bdfca99865f808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ds_yelp.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_yelp[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b261763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train samples: 10,000\n",
      "Tokenized test samples: 2,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenized train samples: {len(tokenized_datasets['train']):,}\")\n",
    "print(f\"Tokenized test samples: {len(tokenized_datasets['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b6987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set for evaluation during training and final testing\n",
    "test_size = len(tokenized_datasets[\"test\"])\n",
    "ho_size = min(500, test_size // 2)\n",
    "\n",
    "ho_dataset = tokenized_datasets[\"test\"].select(range(ho_size))\n",
    "final_test_dataset = tokenized_datasets[\"test\"].select(range(ho_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1ff5fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset: 500 samples\n",
      "Final test dataset: 1500 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation dataset: {len(ho_dataset)} samples\")\n",
    "print(f\"Final test dataset: {len(final_test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1de32",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "Perform an initial evaluation of the model on your chosen sequence classification task. This step will require that you also load an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823e96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, batch_size=16):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def evaluate_model_with_trainer(model, test_dataset, model_name=\"model\"):\n",
    "    \"\"\"Evaluate model performance using HuggingFace Trainer\"\"\"\n",
    "    print(f\"Evaluating {model_name} using HuggingFace Trainer...\")\n",
    "    \n",
    "    # Define compute metrics function\n",
    "    def compute_metrics_eval(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        return {\"accuracy\": accuracy}\n",
    "    \n",
    "    # Create temporary evaluation arguments\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=f\"./temp_eval_{model_name}\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        dataloader_drop_last=False,\n",
    "        report_to=None,\n",
    "    )\n",
    "    \n",
    "    # Create trainer for evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics_eval,\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Get predictions for detailed analysis\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(true_labels, pred_labels, \n",
    "                              target_names=[f\"{i+1} stars\" for i in range(5)]))\n",
    "    \n",
    "    return eval_results['eval_accuracy'], pred_labels, true_labels\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     for batch in dataloader:\n",
    "    #         # Move batch to device\n",
    "    #         input_ids = batch[\"input_ids\"]\n",
    "    #         attention_mask = batch[\"attention_mask\"]\n",
    "    #         labels = batch[\"labels\"]\n",
    "            \n",
    "    #         # Forward pass\n",
    "    #         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    #         predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "    #         all_predictions.extend(predictions.cpu().numpy())\n",
    "    #         all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # return all_predictions, all_labels\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for i, batch in enumerate(dataloader):\n",
    "    #         try:\n",
    "    #             # Debug: Print batch info for first iteration\n",
    "    #             if i == 0:\n",
    "    #                 print(f\"Batch keys: {batch.keys()}\")\n",
    "    #                 print(f\"input_ids type: {type(batch['input_ids'])}\")\n",
    "    #                 print(f\"input_ids shape: {batch['input_ids'].shape if hasattr(batch['input_ids'], 'shape') else 'No shape attribute'}\")\n",
    "    #                 print(f\"labels type: {type(batch['labels'])}\")\n",
    "                \n",
    "    #             # Convert to tensors if they're not already\n",
    "    #             if isinstance(batch[\"input_ids\"], list):\n",
    "    #                 input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "    #             else:\n",
    "    #                 input_ids = batch[\"input_ids\"].to(device)\n",
    "                \n",
    "    #             if isinstance(batch[\"attention_mask\"], list):\n",
    "    #                 attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "    #             else:\n",
    "    #                 attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                \n",
    "    #             if isinstance(batch[\"labels\"], list):\n",
    "    #                 labels = torch.tensor(batch[\"labels\"])\n",
    "    #             else:\n",
    "    #                 labels = batch[\"labels\"]\n",
    "                \n",
    "    #             # Debug: Print tensor shapes for first iteration\n",
    "    #             if i == 0:\n",
    "    #                 print(f\"Processed input_ids shape: {input_ids.shape}\")\n",
    "    #                 print(f\"Processed attention_mask shape: {attention_mask.shape}\")\n",
    "    #                 print(f\"Processed labels shape: {labels.shape}\")\n",
    "                \n",
    "    #             # Forward pass\n",
    "    #             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    #             predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                \n",
    "    #             # Convert to numpy for storage\n",
    "    #             if isinstance(predictions, torch.Tensor):\n",
    "    #                 all_predictions.extend(predictions.cpu().numpy())\n",
    "    #             else:\n",
    "    #                 all_predictions.extend(predictions)\n",
    "                \n",
    "    #             if isinstance(labels, torch.Tensor):\n",
    "    #                 all_labels.extend(labels.cpu().numpy())\n",
    "    #             else:\n",
    "    #                 all_labels.extend(labels)\n",
    "                \n",
    "    #             # Progress indicator\n",
    "    #             if i % 10 == 0 and i > 0:\n",
    "    #                 print(f\"Processed {i * batch_size} samples...\")\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error in batch {i}: {str(e)}\")\n",
    "    #             print(f\"Batch content: {batch}\")\n",
    "    #             raise e\n",
    "    \n",
    "    # print(f\"Evaluation completed. Total predictions: {len(all_predictions)}\")\n",
    "    # return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de2983a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Base GPT-2 using HuggingFace Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/8jf5n46j4mgcn25c47w2wxrr0000gn/T/ipykernel_64965/1597688852.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 Accuracy: 0.1933\n",
      "\n",
      "Base GPT-2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     1 stars       0.19      1.00      0.32       289\n",
      "     2 stars       0.00      0.00      0.00       288\n",
      "     3 stars       0.00      0.00      0.00       311\n",
      "     4 stars       0.00      0.00      0.00       332\n",
      "     5 stars       0.25      0.00      0.01       280\n",
      "\n",
      "    accuracy                           0.19      1500\n",
      "   macro avg       0.09      0.20      0.07      1500\n",
      "weighted avg       0.08      0.19      0.06      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base model\n",
    "base_accuracy, base_predictions, base_labels = evaluate_model_with_trainer(\n",
    "    model, final_test_dataset, \"Base GPT-2\")\n",
    "\n",
    "# base_predictions, base_labels = evaluate_model(model, final_test_dataset)\n",
    "\n",
    "# # Calculate metrics\n",
    "# base_accuracy = accuracy_score(base_labels, base_predictions)\n",
    "\n",
    "# print(f\"Base GPT-2 Accuracy: {base_accuracy:.4f}\")\n",
    "# print(\"\\nBase Model Classification Report:\")\n",
    "# print(classification_report(base_labels, base_predictions, \n",
    "#                           target_names=[f\"{i+1} stars\" for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "551388f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# def compute_metrics_beta(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fe354",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "### Creating a PEFT config\n",
    "\n",
    "Create a PEFT config with appropriate hyperparameters for your chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "817f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence Classification\n",
    "    inference_mode=False,\n",
    "    r=8,  # Rank of adaptation\n",
    "    lora_alpha=32,  # LoRA scaling parameter\n",
    "    lora_dropout=0.1,  # LoRA dropout\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Target modules for GPT-2\n",
    ")\n",
    "\n",
    "# config = LoraConfig()\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bd4cc",
   "metadata": {},
   "source": [
    "### Creating a PEFT model\n",
    "\n",
    "Using the PEFT config and foundation model, create a PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3585f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 814,848 || all params: 125,258,496 || trainable%: 0.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a4eff98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ceb07d",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Using the PEFT model and dataset, run a training loop with at least one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1442b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_gpt2_yelp\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./lora_gpt2_yelp/logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    use_mps_device=True,\n",
    "    label_names=train_df[\"label\"].unique(),\n",
    "    report_to=None,  # Disable wandb logging\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=ho_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=TrainingArguments(\n",
    "#         output_dir=\"./data/sentiment_analysis\",\n",
    "#         learning_rate=2e-3,\n",
    "#         # Reduce the batch size if you don't have enough memory\n",
    "#         per_device_train_batch_size=4,\n",
    "#         per_device_eval_batch_size=4,\n",
    "#         num_train_epochs=1,\n",
    "#         weight_decay=0.01,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#     ),\n",
    "#     train_dataset=tokenized_ds[\"train\"],\n",
    "#     eval_dataset=tokenized_ds[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     # data_collator=DataCollatorWithPadding(<MASK>),\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer = tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201d76e",
   "metadata": {},
   "source": [
    "### Saving the trained model\n",
    "\n",
    "Depending on your training loop configuration, your PEFT model may have already been saved. If not, use `save_pretrained` to save your progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f4ddc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U9'), dtype('int64')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/trainer.py:2270\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2268\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2270\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2272\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/trainer.py:1011\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1009\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m-> 1011\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/trainer.py:922\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mremove_unused_columns:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_signature_columns_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m signature_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns\n\u001b[1;32m    925\u001b[0m ignored_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumn_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(signature_columns))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/trainer.py:917\u001b[0m, in \u001b[0;36mTrainer._set_signature_columns_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Labels may be named label or label_ids, the default data collator handles that.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_names\u001b[49m))\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U9'), dtype('int64')) -> None"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79047c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91507c9b",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "## Loading the model\n",
    "\n",
    "Using the appropriate PEFT model class, load your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed45500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337bb47b",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Repeat the previous evaluation process, this time using the PEFT model. Compare the results to the results from the original foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b6988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
