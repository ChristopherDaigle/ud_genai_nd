{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e43131f",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "**Project Introduction**\n",
    "\n",
    "Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows you to modify foundation models for your needs without needing substantial computational resources.\n",
    "\n",
    "In this project, you will apply parameter-efficient fine-tuning using the Hugging Face `peft` library.\n",
    "\n",
    "**Project Summary**\n",
    "\n",
    "In this project, you will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, you will:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb974d1",
   "metadata": {},
   "source": [
    "## HuggingFace PEFT Library\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "Hugging Face PEFT allows you to fine-tune a model without having to fine-tune all of its parameters.\n",
    "\n",
    "Training a model using Hugging Face PEFT requires two additional steps beyond traditional fine-tuning:\n",
    "\n",
    "1. Creating a **PEFT config**\n",
    "2. **Converting the model into a PEFT model** using the PEFT config\n",
    "Inference using a PEFT model is almost identical to inference using a non-PEFT model. The only difference is that it must be loaded as a PEFT model.\n",
    "\n",
    "## Training with PEFT\n",
    "\n",
    "Creating a PEFT Config\n",
    "The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a `PeftConfig`, but this example will use a `LoraConfig`, the subclass used for low rank adaptation (LoRA).\n",
    "\n",
    "A LoRA config can be instantiated like this:\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "config = LoraConfig()\n",
    "```\n",
    "\n",
    "Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to `LoraConfig()`. [Hugging Face LoRA conceptual guide](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) also contains additional explanations.\n",
    "\n",
    "## Converting a Transformers Model into a PEFT Model\n",
    "\n",
    "Once you have a PEFT config object, you can load a Hugging Face `transformers` model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "```\n",
    "\n",
    "Then using `get_peft_model()` to get a trainable PEFT model (using the LoRA config instantiated previously):\n",
    "\n",
    "```python\n",
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)\n",
    "```\n",
    "\n",
    "## Training with a PEFT Model\n",
    "\n",
    "After calling `get_peft_model()`, you can then use the resulting `lora_model` in a training process of your choice (PyTorch training loop or Hugging Face `Trainer`).\n",
    "\n",
    "## Checking Trainable Parameters of a PEFT Model\n",
    "\n",
    "A helpful way to check the number of trainable parameters with the current config is the `print_trainable_parameters()` method:\n",
    "\n",
    "```python\n",
    "lora_model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "Which prints an output like this:\n",
    "\n",
    "`trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364`\n",
    "\n",
    "## Saving a Trained PEFT Model\n",
    "\n",
    "Once a PEFT model has been trained, the standard Hugging Face `save_pretrained()` method can be used to save the weights locally. For example:\n",
    "\n",
    "```python\n",
    "lora_model.save_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "Note that this **only saves the adapter weights** and not the weights of the original Transformers model. Thus the size of the files created will be much smaller than you might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42be92",
   "metadata": {},
   "source": [
    "# Inference with PEFT\n",
    "\n",
    "## Loading a Saved PEFT Model\n",
    "\n",
    "Because you have only saved the adapter weights and not the full model weights, you can't use `from_pretrained()` with the regular Transformers class (e.g., `AutoModelForCausalLM`). Instead, you need to use the PEFT version (e.g., `AutoPeftModelForCausalLM`). For example:\n",
    "\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "lora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora\")\n",
    "```\n",
    "\n",
    "After completing this step, you can proceed to use the model for inference.\n",
    "\n",
    "# Generating Text from a PEFT Model\n",
    "\n",
    "You may see examples from regular Transformer models where the input IDs are passed in as a positional argument (e.g., `model.generate(input_ids)`). For a PEFT model, they must be passed in as a keyword argument (e.g., `model.generate(input_ids=input_ids)`). For example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "```\n",
    "\n",
    "**Documentation Links**\n",
    "\n",
    "* [Hugging Face PEFT configuration](https://huggingface.co/docs/peft/package_reference/config)\n",
    "* [Hugging Face LoRA adapter](https://huggingface.co/docs/peft/package_reference/lora)\n",
    "* [Hugging Face Models save_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)\n",
    "* [Hugging Face Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347a791",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
