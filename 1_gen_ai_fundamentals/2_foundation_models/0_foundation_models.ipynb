{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4572f5c-6bcf-43d9-857a-8f02eb9ef5ed",
   "metadata": {},
   "source": [
    "# What is a Foundation Model\n",
    "\n",
    "A foundation model is a powerful AI tool that can do many different things after being trained on lots of diverse data. These models are incredibly versatile and provide a solid base for creating various AI applications, like a strong foundation holds up different kind of buildings. By using a foundation model, we have a strong starting point for building specialized AI tasks.\n",
    "\n",
    "\n",
    "## Terms Explained:\n",
    "\n",
    "* **Foundation Model**: A large AI model trained on a wide variety of data, which can do many tasks without much extra training.\n",
    "\n",
    "* **Adapted**: Modified or adjusted to suit new conditions or a new purpose, i.e. in the context of foundation models.\n",
    "\n",
    "* **Generalize**: The ability of a model to apply what it has learned from its training data to new, unseen data.\n",
    "\n",
    "Foundation models represent a paradigm shift in building machine learning systems. In the past, there was more of a focus on building large datasets and building models from scratch. With the advent of foundation models, the pattern has changed. Now, people generally start with the foundation model and build off of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956ac2c-b177-4d7d-be6d-f78ef351a280",
   "metadata": {},
   "source": [
    "# Foundation Models vs Traditional Models\n",
    "\n",
    "Foundation Models and Traditional Models are two distinct approaches in the field of artificial intelligence with different strengths. Foundation Models, which are built on large, diverse datasets, have the incredible ability to adapt and perform well on many different tasks. In contrast, Traditional Models specialize in specific tasks by learning from smaller, focused datasets, making them more straightforward and efficient for targeted applications.\n",
    "\n",
    "<img src=\"img/img_00.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb1671-dc84-4826-bbc4-6cef43e64bfb",
   "metadata": {},
   "source": [
    "# Architecture and Scale\n",
    "\n",
    "The transformer architecture has revolutionized the way machines handle language by enabling the training of sequential data at scale. Thanks to this, todayâ€™s AI models are massive, with some having billions of parameters (or more) allowing for incredible flexibility across many tasks. The technology is exciting and holds great promise for the future.\n",
    "\n",
    "## Technical Terms:\n",
    "\n",
    "* **Sequential data**: Information that is arranged in a specific order, such as words in a sentence or events in time.\n",
    "\n",
    "* **Self-attention mechanism**: The self-attention mechanism in a transformer is a process where each element in a sequence computes its representation by attending to and weighing the importance of all elements in the sequence, allowing the model to capture complex relationships and dependencies.\n",
    "\n",
    "<img src=\"img/img_01.png\">\n",
    "\n",
    "The initial LLaMa from Meta was trained on more than 4.7TB of Data\n",
    "\n",
    "<img src=\"img/img_02.png\">\n",
    "\n",
    "Traditional models typically have between 3 and 20 paramaters, each associated with their input variables (1 parameter for $X_{0}$ , another for $X_{1}$, all the way through $X_{n}$ such that for each $X_{i}$ there is a $\\beta_{i}$ ) while foundation models generally have billions of paramters spread over the layers in their networks.\n",
    "\n",
    "<img src=\"img/img_03.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abcb3b-e8a9-4b26-a55f-6afee4212656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "venv_ud_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
