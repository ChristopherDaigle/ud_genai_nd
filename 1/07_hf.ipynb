{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbd9c0a-307b-45f4-9b23-9e32dbb339d8",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "Hugging Face offers everything from tokenizers, which help computers make sense of text, to a huge variety of ready-to-go language models, and even a treasure trove of data suited for language tasks.\n",
    "\n",
    "HF provides many things, some of which are:\n",
    "1. Tokenizers\n",
    "2. Models\n",
    "3. Datasets\n",
    "4. Trainers\n",
    "\n",
    "**Tokenizers:** These work like a translator, converting the words we use into smaller parts and creating a secret code that computers can understand and work with.\n",
    "\n",
    "**Models:** These are like the brain for computers, allowing them to learn and make decisions based on information they've been fed.\n",
    "\n",
    "**Datasets:** Think of datasets as textbooks for computer models. They are collections of information that models study to learn and improve.\n",
    "\n",
    "**Trainers:** Trainers are the coaches for computer models. They help these models get better at their tasks by practicing and providing guidance. HuggingFace Trainers implement the PyTorch training loop for you, so you can focus instead on other aspects of working on the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6391b94-2052-42b2-b3ce-eed91e9b01d1",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "HuggingFace tokenizers help us break down text into smaller, manageable pieces called tokens. These tokenizers are easy to use and also remarkably fast due to their use of the Rust programming language.\n",
    "\n",
    "**Tokenization:** The process by which an input series of characters is transformed into units the model is prepared to predict upon. A model trained on data tokenized by one tokenizer must use that same tokenizer for prediction; this is similar to feature engineering in traditional machine learning. It's like cutting a sentence into individual pieces, such as words or characters, to make it easier to analyze.\n",
    "\n",
    "**Tokens:** Fundamental unit of input to language models. These are the pieces you get after cutting up text during tokenization, kind of like individual Lego blocks that can be words, parts of words, or even single letters. These tokens are converted to numerical values for models to understand.\n",
    "\n",
    "**Pre-trained Model:** This is a ready-made model that has been previously taught with a lot of data.\n",
    "\n",
    "**Uncased:** This means that the model treats uppercase and lowercase letters as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71eac3c-ff34-4ec2-93ff-ec22e5ba39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e6fb4c-eab1-4c2e-8dd4-4b54c583683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c6099-7a5a-4d54-8c6c-a5c81e7efa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many tokens are in the vocabulary\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a197dd-9331-4b55-8633-520abcb765ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "sent_0 = \"I heart Generative AI\"\n",
    "tokens = tokenizer.tokenize(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91524b6-bc36-4c90-b4e1-c1eb71f16428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'heart', 'genera', '##tive', 'ai']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcd8be4-1d4d-4c17-9d6f-babe6abb1074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 2540, 11416, 6024, 9932]\n"
     ]
    }
   ],
   "source": [
    "# Show the token ids assigned to each token\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1a6712-02da-4ea5-940c-3a00b3a9bde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1045, 'heart': 2540, 'genera': 11416, '##tive': 6024, 'ai': 9932}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(tokens,tokenizer.convert_tokens_to_ids(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddb44f-bd45-4f4d-87c2-041ab6686068",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Hugging Face models provide a quick way to get started using models trained by the community. With only a few lines of code, you can load a pre-trained model and start using it on tasks such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d215935-fcaf-4604-9911-ab8be60255e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850c78be-c7fc-493b-8375-418e630a1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a7395d-66d0-41e9-8e8f-d84fe8f86362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1045, 'love': 2293, 'mathematics': 5597}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input sequence\n",
    "sent = \"I love mathematics\"\n",
    "inputs = tokenizer.tokenize(text=sent)\n",
    "dict(zip(inputs,tokenizer.convert_tokens_to_ids(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3326ba-0560-41d6-a734-ab5ffb121b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def use_model(input_text):\n",
    "    inputs = tokenizer(\n",
    "        text=input_text,\n",
    "        return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities)\n",
    "    if predicted_class == 1:\n",
    "        print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "    label = model.config.id2label[predicted_class.item()]\n",
    "    arg_ind = predicted_class.item()\n",
    "    print(f\"\\tModel label: {label}\")\n",
    "    print(f\"\\tModel arg_index: {arg_ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0dabe78-d82c-4cad-a480-12aab360b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e1576f-4dde-42a7-b6f7-6eaf2c09e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively:\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83a28f2-3150-4a79-8595-ab6030413bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b258c530-5fc6-4ce7-ba2d-dc193d854245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Alternatively:\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aeb2832-3ef0-46da-a5a1-92d64a7135fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_1', 'score': 0.8251070380210876}\n",
      "\n",
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel score: 0.8251070380210876\n"
     ]
    }
   ],
   "source": [
    "out = pipe(sent)\n",
    "print(out[0])\n",
    "print()\n",
    "label = out[0]['label']\n",
    "score = out[0]['score']\n",
    "if label == \"LABEL_1\":\n",
    "    print(f\"Sentiment: Positive ({score * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({score * 100:.2f}%)\")\n",
    "# label = model.config.id2label[predicted_class.item()]\n",
    "# arg_ind = predicted_class.item()\n",
    "print(f\"\\tModel label: {label}\")\n",
    "print(f\"\\tModel score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe7f10-9d0c-42aa-a573-49b7c079f960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "venv_ud_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
