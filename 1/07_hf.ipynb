{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbd9c0a-307b-45f4-9b23-9e32dbb339d8",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "Hugging Face offers everything from tokenizers, which help computers make sense of text, to a huge variety of ready-to-go language models, and even a treasure trove of data suited for language tasks.\n",
    "\n",
    "HF provides many things, some of which are:\n",
    "1. Tokenizers\n",
    "2. Models\n",
    "3. Datasets\n",
    "4. Trainers\n",
    "\n",
    "**Tokenizers:** These work like a translator, converting the words we use into smaller parts and creating a secret code that computers can understand and work with.\n",
    "\n",
    "**Models:** These are like the brain for computers, allowing them to learn and make decisions based on information they've been fed.\n",
    "\n",
    "**Datasets:** Think of datasets as textbooks for computer models. They are collections of information that models study to learn and improve.\n",
    "\n",
    "**Trainers:** Trainers are the coaches for computer models. They help these models get better at their tasks by practicing and providing guidance. HuggingFace Trainers implement the PyTorch training loop for you, so you can focus instead on other aspects of working on the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6391b94-2052-42b2-b3ce-eed91e9b01d1",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "HuggingFace tokenizers help us break down text into smaller, manageable pieces called tokens. These tokenizers are easy to use and also remarkably fast due to their use of the Rust programming language.\n",
    "\n",
    "**Tokenization:** The process by which an input series of characters is transformed into units the model is prepared to predict upon. A model trained on data tokenized by one tokenizer must use that same tokenizer for prediction; this is similar to feature engineering in traditional machine learning. It's like cutting a sentence into individual pieces, such as words or characters, to make it easier to analyze.\n",
    "\n",
    "**Tokens:** Fundamental unit of input to language models. These are the pieces you get after cutting up text during tokenization, kind of like individual Lego blocks that can be words, parts of words, or even single letters. These tokens are converted to numerical values for models to understand.\n",
    "\n",
    "**Pre-trained Model:** This is a ready-made model that has been previously taught with a lot of data.\n",
    "\n",
    "**Uncased:** This means that the model treats uppercase and lowercase letters as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71eac3c-ff34-4ec2-93ff-ec22e5ba39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e6fb4c-eab1-4c2e-8dd4-4b54c583683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c6099-7a5a-4d54-8c6c-a5c81e7efa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many tokens are in the vocabulary\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a197dd-9331-4b55-8633-520abcb765ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "sent_0 = \"I heart Generative AI\"\n",
    "tokens = tokenizer.tokenize(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91524b6-bc36-4c90-b4e1-c1eb71f16428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'heart', 'genera', '##tive', 'ai']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcd8be4-1d4d-4c17-9d6f-babe6abb1074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 2540, 11416, 6024, 9932]\n"
     ]
    }
   ],
   "source": [
    "# Show the token ids assigned to each token\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1a6712-02da-4ea5-940c-3a00b3a9bde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1045, 'heart': 2540, 'genera': 11416, '##tive': 6024, 'ai': 9932}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(tokens,tokenizer.convert_tokens_to_ids(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddb44f-bd45-4f4d-87c2-041ab6686068",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Hugging Face models provide a quick way to get started using models trained by the community. With only a few lines of code, you can load a pre-trained model and start using it on tasks such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d215935-fcaf-4604-9911-ab8be60255e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850c78be-c7fc-493b-8375-418e630a1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a7395d-66d0-41e9-8e8f-d84fe8f86362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1045, 'love': 2293, 'mathematics': 5597}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input sequence\n",
    "sent = \"I love mathematics\"\n",
    "inputs = tokenizer.tokenize(text=sent)\n",
    "dict(zip(inputs,tokenizer.convert_tokens_to_ids(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3326ba-0560-41d6-a734-ab5ffb121b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def use_model(input_text):\n",
    "    inputs = tokenizer(\n",
    "        text=input_text,\n",
    "        return_tensors=\"pt\")\n",
    "    # Get predictions without updating the model - the no_grad method means no updating of the gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities)\n",
    "    if predicted_class == 1:\n",
    "        print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "    label = model.config.id2label[predicted_class.item()]\n",
    "    arg_ind = predicted_class.item()\n",
    "    print(f\"\\tModel label: {label}\")\n",
    "    print(f\"\\tModel arg_index: {arg_ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0dabe78-d82c-4cad-a480-12aab360b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e1576f-4dde-42a7-b6f7-6eaf2c09e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively:\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83a28f2-3150-4a79-8595-ab6030413bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b258c530-5fc6-4ce7-ba2d-dc193d854245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Alternatively:\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aeb2832-3ef0-46da-a5a1-92d64a7135fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_1', 'score': 0.8251070380210876}\n",
      "\n",
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel score: 0.8251070380210876\n"
     ]
    }
   ],
   "source": [
    "out = pipe(sent)\n",
    "print(out[0])\n",
    "print()\n",
    "label = out[0]['label']\n",
    "score = out[0]['score']\n",
    "if label == \"LABEL_1\":\n",
    "    print(f\"Sentiment: Positive ({score * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({score * 100:.2f}%)\")\n",
    "# label = model.config.id2label[predicted_class.item()]\n",
    "# arg_ind = predicted_class.item()\n",
    "print(f\"\\tModel label: {label}\")\n",
    "print(f\"\\tModel score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20351aae-12a4-48a2-910c-cbaf7d9b5fd4",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "HuggingFace Datasets library is a powerful tool for managing a variety of data types, like text and images, efficiently and easily. This resource is incredibly fast and doesn't use a lot of computer memory, making it great for handling big projects without any hassle.\n",
    "\n",
    "**IMDb dataset:** A dataset of movie reviews that can be used to train a machine learning model to understand human sentiments.\n",
    "\n",
    "**Apache Arrow:** A software framework that allows for fast data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4dfa94e-03a7-495d-9bc7-418e0bb7e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets huggingface_hub\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80b24043-d121-4e50-a604-4dbd0b2d24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = huggingface_hub.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd920f4-93fa-40ca-894b-e77258e23d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object HfApi.list_datasets at 0x3addc1380>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8271729-a728-4774-b9d1-7022717295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_ds = list(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c28b5e11-7540-4517-b76c-85b876062429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(id='fka/awesome-chatgpt-prompts', author='fka', sha='68ba7694e23014788dcc8ab5afe613824f45a05c', created_at=datetime.datetime(2022, 12, 13, 23, 47, 45, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2025, 1, 6, 0, 2, 53, tzinfo=datetime.timezone.utc), private=False, gated=False, disabled=False, downloads=6066, downloads_all_time=None, likes=6941, paperswithcode_id=None, tags=['task_categories:question-answering', 'license:cc0-1.0', 'size_categories:n<1K', 'format:csv', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us', 'ChatGPT'], trending_score=131, card_data=None, siblings=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb2c834-cf2b-4313-bc1d-c80c46d17dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the IMDB dataset, which contains movie reviews and sentiment labels (positive or negative)\n",
    "ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c7c41f-76f9-48d2-b4b2-ec339d3e68e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve a single review\n",
    "rev_num = 42\n",
    "sample_rev = ds[\"train\"][rev_num]\n",
    "display(HTML(sample_rev[\"text\"][:450] + \"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4395ea09-e6a4-41eb-8ef9-c7ab7795492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rev.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61e7e27d-7bdb-431d-a35a-d7527ee5b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "if sample_rev[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n",
    "# Sentiment: Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcd361-75d7-4335-9888-e331799ef1db",
   "metadata": {},
   "source": [
    "## Trainers\n",
    "\n",
    "[HuggingFace trainers](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#trainer) offer a simplified approach to training generative AI models, making it easier to set up and run complex machine learning tasks. This tool wraps up the hard parts, like handling data and carrying out the training process, allowing us to focus on the big picture and achieve better outcomes with our AI endeavors.\n",
    "\n",
    "**Truncating:** This refers to shortening longer pieces of text to fit a certain size limit.\n",
    "\n",
    "**Padding:** Adding extra data to shorter texts to reach a uniform length for processing.\n",
    "\n",
    "**Batches:** Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.\n",
    "\n",
    "**Batch Size:** The number of data samples that the machine considers in one go during training.\n",
    "\n",
    "**Epochs:** A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.\n",
    "\n",
    "**Dataset Splits:** Dividing the dataset into parts for different uses, such as training the model and testing how well it works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a1e0d2-a96a-4edb-bbcf-19dffb0cb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dea858fc-221f-44bb-b4bd-3a0b2377f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForSequenceClassification\n",
    "tokenizer_1 = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_1(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b5286c-e63c-4e90-8dbe-e9b17e1db134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"imdb\")\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f39d58d6-c6a8-41e5-ac02-1c84efba3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chk = DistilBertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./results/checkpoint-1173\"\n",
    "    ,num_labels=2\n",
    ")\n",
    "# \"./results/mps_1\"\n",
    "# \"./results\"\n",
    "# Make prediction\n",
    "def use_model_chk(input_text):\n",
    "    inputs = tokenizer_1(\n",
    "        text=input_text,\n",
    "        return_tensors=\"pt\")\n",
    "    # Get predictions without updating the model - the no_grad method means no updating of the gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model_chk(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities)\n",
    "    if predicted_class == 1:\n",
    "        print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "    label = model_chk.config.id2label[predicted_class.item()]\n",
    "    arg_ind = predicted_class.item()\n",
    "    print(f\"\\tModel label: {label}\")\n",
    "    print(f\"\\tModel arg_index: {arg_ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "874268a6-e04e-47be-907d-794e53507536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love mathematics\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bfe7006-10c7-452b-b9cd-167198030fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (93.09%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model_chk(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8eca880c-4aab-468e-bb63-aa6d3c4cf0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (82.51%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "use_model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b755b8b4-f840-4877-9b2e-340644916d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chk = DistilBertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./results/mps/checkpoint-1173\"\n",
    "    ,num_labels=2\n",
    ")\n",
    "# \"./results/mps_1\"\n",
    "# \"./results\"\n",
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8877bdc8-d35b-4ba2-a42a-d91922fe42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (98.04%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    }
   ],
   "source": [
    "use_model_chk(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27737c11-4ba0-45ed-b9bc-dd0ff7a91fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69f97434-7646-4eeb-99f6-890f816a50c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 38:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1173, training_loss=0.18140956101413477, metrics={'train_runtime': 2333.48, 'train_samples_per_second': 32.141, 'train_steps_per_second': 0.503, 'total_flos': 9935054899200000.0, 'train_loss': 0.18140956101413477, 'epoch': 3.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7fc74365-5f8f-4c14-8ef2-edad1f29894e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 39:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainOutput' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/mps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     10\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainOutput' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    output_dir=\"./results/mps\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer.train().to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fab5a7f-1673-4b3b-bac8-3d82a2a3a3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherdaigle/miniconda3/envs/venv_ud_gen/lib/python3.10/site-packages/transformers/training_args.py:2255: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='196' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [196/196 52:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=196, training_loss=0.046327522822788784, metrics={'train_runtime': 3151.4722, 'train_samples_per_second': 7.933, 'train_steps_per_second': 0.062, 'total_flos': 3311684966400000.0, 'train_loss': 0.046327522822788784, 'epoch': 1.0})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=128,\n",
    "    output_dir=\"./results/mps_1\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    use_mps_device=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41d77371-29e5-4ad6-be00-e14d2c744e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9acc43b-f8f5-466f-b12a-0b919cb599ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu w/ mem write 0.7389892083592713\n",
      "\tcpu w/ NO mem write 0.07051066681742668\n",
      "mps  w/ mem write 2.83012962481007\n",
      "\tmps  w/ NO mem write 2.426200541201979\n"
     ]
    }
   ],
   "source": [
    "a_cpu = torch.rand(250, device='cpu')\n",
    "b_cpu = torch.rand((250, 250), device='cpu')\n",
    "a_mps = torch.rand(250, device='mps')\n",
    "b_mps = torch.rand((250, 250), device='mps')\n",
    "\n",
    "print('cpu w/ mem write', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000))\n",
    "print('\\tcpu w/ NO mem write', timeit.timeit(lambda: a_cpu @ a_cpu, number=100_000))\n",
    "print('mps  w/ mem write', timeit.timeit(lambda: a_mps @ b_mps, number=100_000))\n",
    "print('\\tmps  w/ NO mem write', timeit.timeit(lambda: a_mps @ a_mps, number=100_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76b34a63-1166-4112-966f-de074ff506f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d87110-0b1c-4139-92cf-13a0f08f33f8",
   "metadata": {},
   "source": [
    "# Exercise: PyTorch and HuggingFace scavenger hunt!\n",
    "\n",
    "PyTorch and HuggingFace have emerged as powerful tools for developing and deploying neural networks.\n",
    "\n",
    "In this scavenger hunt, we will explore the capabilities of PyTorch and HuggingFace, uncovering hidden treasures on the way.\n",
    "\n",
    "We have two parts:\n",
    "* Familiarize yourself with PyTorch\n",
    "* Get to know HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb94f7a-2665-4cf9-b820-cb9b8e4db2c9",
   "metadata": {},
   "source": [
    "## Familiarize yourself with PyTorch\n",
    "\n",
    "Learn the basics of PyTorch, including tensors, neural net parts, loss functions, and optimizers. This will provide a foundation for understanding and utilizing its capabilities in developing and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703acc2-cc2f-4f0e-96e2-c4b90348462e",
   "metadata": {},
   "source": [
    "### PyTorch tensors\n",
    "\n",
    "Scan through the PyTorch tensors documentation [here](https://pytorch.org/docs/stable/tensors.html). Be sure to look at the examples.\n",
    "\n",
    "In the following cell, create a tensor named `my_tensor` of size 3x3 with values of your choice. The tensor should be created on the GPU if available. Print the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a9e09f-ae01-4e0a-ac25-e0b507ceafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586dd81a-0be4-42b7-8847-106c42a3e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Fill in the missing parts labelled <MASK> with the appropriate code to complete the exercise.\n",
    "# Hint: Use torch.cuda.is_available() to check if GPU is available\n",
    "\n",
    "# Set the device to be used for the tensor\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ffd23f-009e-4562-9a64-0cbb9e32f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8857, -0.2106,  0.6445],\n",
      "        [ 0.8209, -0.4323, -0.0288],\n",
      "        [-0.2345,  0.2012,  1.6298]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor on the appropriate device\n",
    "# my_tensor = <MASK>\n",
    "my_tensor = torch.randn((3, 3))\n",
    "# # Print the tensor\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c502f3-1c92-48b5-80bb-620f3328a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check the previous cell\n",
    "# assert my_tensor.device.type in {\"cuda\", \"cpu\"}\n",
    "assert my_tensor.device.type in {\"mps\", \"cpu\"}\n",
    "assert my_tensor.shape == (3, 3)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e544eaa-8ae6-427e-9c75-beb234289fb7",
   "metadata": {},
   "source": [
    "### Neural Net Constructor Kit `torch.nn`\n",
    "\n",
    "You can think of the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html)) module as a constructor kit for neural networks. It provides the building blocks for creating neural networks, including layers, activation functions, loss functions, and more.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Create a three layer Multi-Layer Perceptron (MLP) neural network with the following specifications:\n",
    "\n",
    "- Input layer: 784 neurons\n",
    "- Hidden layer: 128 neurons\n",
    "- Output layer: 10 neurons\n",
    "\n",
    "Use the ReLU activation function for the hidden layer and the softmax activation function for the output layer. Print the neural network.\n",
    "\n",
    "Hint: MLP's use \"fully-connected\" or \"dense\" layers. In PyTorch's `nn` module, this type of layer has a different name. See the examples in [this tutorial](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51769250-d2cf-459c-ba3a-dbba97d5bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc73f43a-159d-4f44-8047-027db5e86665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_size:int=784):\n",
    "        \"\"\"My Multilayer Perceptron (MLP)\n",
    "    \n",
    "        Specifications:\n",
    "    \n",
    "            - Input layer: 784 neurons\n",
    "            - Hidden layer: 128 neurons with ReLU activation\n",
    "            - Output layer: 10 neurons with softmax activation\n",
    "    \n",
    "        \"\"\"\n",
    "        super(MyMLP, self).__init__()\n",
    "        # self.fc1 = <MASK>\n",
    "        # self.fc2 = <MASK>\n",
    "        # self.relu = <MASK>\n",
    "        # self.softmax = <MASK>\n",
    "\n",
    "        # self.hidden_layer = nn.Linear(in_features=input_size, out_features=128)\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=128)\n",
    "        # self.output_layer = nn.Linear(128, 10)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        # self.activation = nn.ReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        # Single dimension input to the softmax layer, so dim=0\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        # https://discuss.pytorch.org/t/implicit-dimension-choice-for-softmax-warning/12314/17\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # Pass the input to the second layer\n",
    "        # x = <MASK>\n",
    "        # # Apply ReLU activation\n",
    "        # x = <MASK>\n",
    "        # # Pass the result to the final layer\n",
    "        # x = <MASK>\n",
    "        # # Apply softmax activation\n",
    "        # x = <MASK>\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f59a9c2-808d-4a9c-9ceb-47719dc38a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_mlp = MyMLP()\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5b375e-6743-4970-ba54-6c3e9e35728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check your work here:\n",
    "\n",
    "\n",
    "# Check the number of inputs\n",
    "assert my_mlp.fc1.in_features == 784\n",
    "\n",
    "# Check the number of outputs\n",
    "assert my_mlp.fc2.out_features == 10\n",
    "\n",
    "# Check the number of nodes in the hidden layer\n",
    "assert my_mlp.fc1.out_features == 128\n",
    "\n",
    "# Check that my_mlp.fc1 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc1, nn.Linear)\n",
    "\n",
    "# Check that my_mlp.fc2 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc2, nn.Linear)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d4e0d-cf4d-4fbf-89ed-7ec08163b138",
   "metadata": {},
   "source": [
    "### PyTorch Loss Functions and Optimizers\n",
    "\n",
    "PyTorch comes with a number of built-in loss functions and optimizers that can be used to train neural networks. The loss functions are implemented in the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)) module, while the optimizers are implemented in the `torch.optim` ([documentation](https://pytorch.org/docs/stable/optim.html)) module.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Create a loss function using the `torch.nn.CrossEntropyLoss` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)) class.\n",
    "- Create an optimizer using the `torch.optim.SGD` ([documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)) class with a learning rate of 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1028663a-c488-4db4-8c43-ceeeffa9302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <MASK> with the appropriate code to complete the exercise.\n",
    "\n",
    "# Loss function\n",
    "# loss_fn = <MASK>\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (by convention we use the variable optimizer)\n",
    "# optimizer = <MASK>\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=my_mlp.parameters()\n",
    "    ,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f76b69-5b71-490e-b1d9-8a5dccc2731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "assert isinstance(\n",
    "    loss_fn, nn.CrossEntropyLoss\n",
    "), \"loss_fn should be an instance of CrossEntropyLoss\"\n",
    "assert isinstance(optimizer, torch.optim.SGD), \"optimizer should be an instance of SGD\"\n",
    "assert optimizer.defaults[\"lr\"] == 0.01, \"learning rate should be 0.01\"\n",
    "assert optimizer.param_groups[0][\"params\"] == list(\n",
    "    my_mlp.parameters()\n",
    "), \"optimizer should be passed the MLP parameters\"\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c631b4-72f2-48fc-9cb6-907691eeb215",
   "metadata": {},
   "source": [
    "### PyTorch Training Loops\n",
    "\n",
    "PyTorch makes writing a training loop easy!\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Fill in the blanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f20bf20-54ad-4c3d-b615-63ddc02ddb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0: 2.30230\n",
      "Epoch 0, batch 10: 2.30265\n",
      "Epoch 0, batch 20: 2.30287\n",
      "Epoch 1, batch 0: 2.30300\n",
      "Epoch 1, batch 10: 2.30227\n",
      "Epoch 1, batch 20: 2.30237\n",
      "Epoch 2, batch 0: 2.30222\n",
      "Epoch 2, batch 10: 2.30209\n",
      "Epoch 2, batch 20: 2.30212\n"
     ]
    }
   ],
   "source": [
    "# Replace <MASK> with the appropriate code to complete the exercise.\n",
    "def fake_training_loaders():\n",
    "    for _ in range(30):\n",
    "        yield torch.randn(64, 784), torch.randint(0, 10, (64,))\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    # Create a training loop\n",
    "    for i, data in enumerate(fake_training_loaders()):\n",
    "        # Every data instance is an input + label pair\n",
    "        x, y = data\n",
    "        # Zero your gradients for every batch!\n",
    "        # <MASK>\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass (predictions)\n",
    "        # y_pred = <MASK>\n",
    "        y_pred = my_mlp(x)\n",
    "        # Compute the loss and its gradients\n",
    "        # loss = <MASK>\n",
    "        # <MASK>\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        # <MASK>\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, batch {i}: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729a1384-f1c2-4a5a-987b-68e0d7ce02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "assert abs(loss.item() - 2.3) < 0.1, \"the loss should be around 2.3 with random data\"\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2acace-c27c-450a-b4d3-c7ec7bd96703",
   "metadata": {},
   "source": [
    "Great job! Now you know the basics of PyTorch! Let's turn to HuggingFace 🤗."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a9bb1-31ed-4f3f-925d-9f5fc81da7de",
   "metadata": {},
   "source": [
    "## Familiarize Yourself With HuggingFace\n",
    "\n",
    "HuggingFace is a popular destination for pre-trained models and datasets that can be applied to a variety of tasks quickly and easily. In this section, we will explore the capabilities of HuggingFace and learn how to use it to build and train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c8f66-a540-40a6-b8a7-b0a707f2da72",
   "metadata": {},
   "source": [
    "### Download a model from HuggingFace and use it for sentiment analysis\n",
    "\n",
    "HuggingFace provides a number of pre-trained models that can be used for a variety of tasks. In this exercise, we will use the `distilbert-base-uncased-finetuned-sst-2-english` model to perform sentiment analysis on a movie review.\n",
    "\n",
    "Instructions:\n",
    "- Review the [AutoModel tutorial](https://huggingface.co/docs/transformers/quicktour#automodel) on the HuggingFace website.\n",
    "- Instantiate an AutoModelForSequenceClassification model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    "- Instantiate an AutoTokenizer using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    "- Define a function that will get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86d83fc-0a4c-417a-8ae3-906014101a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211b8815-a6b0-48bc-bb50-5d2bf3ea2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <MASK> with the appropriate code to complete the exercise.\n",
    "# Get the model and tokenizer\n",
    "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" - This model is causing issues in my kernel, so we'll use the prior model\n",
    "# model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "# pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\", num_labels=2\n",
    "# )\n",
    "# # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForSequenceClassification\n",
    "# tokenizer_1 = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "pt_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./results/checkpoint-1173\"\n",
    "    ,num_labels=2\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_1(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e19bd82-149c-4d5d-857e-d5419343cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(review):\n",
    "    \"\"\"Given a review, return the predicted sentiment\"\"\"\n",
    "\n",
    "    # Tokenize the review\n",
    "    # (Get the response as tensors and not as a list)\n",
    "    # inputs = <MASK>\n",
    "    inputs = tokenizer(\n",
    "        text=review\n",
    "        ,return_tensors=\"pt\")\n",
    "\n",
    "    # Perform the prediction (get the logits)\n",
    "    outputs = pt_model(**inputs)\n",
    "\n",
    "    # Get the predicted class (corresponding to the highest logit)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    return \"positive\" if predictions.item() == 1 else \"negative\"\n",
    "\n",
    "# Make prediction\n",
    "def use_model(input_text,model=pt_model,verbose=0):\n",
    "    inputs = tokenizer(\n",
    "        text=input_text,\n",
    "        return_tensors=\"pt\")\n",
    "    # Get predictions without updating the model - the no_grad method means no updating of the gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities)\n",
    "    label = model.config.id2label[predicted_class.item()]\n",
    "    arg_ind = predicted_class.item()    \n",
    "    if verbose == 1:\n",
    "        if predicted_class == 1:\n",
    "            print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "        print(f\"\\tModel label: {label}\")\n",
    "        print(f\"\\tModel arg_index: {arg_ind}\")\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae5fa7b-6a39-4d97-b5c1-938de61b154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie is not so great :(\n",
      "Sentiment: negative\n",
      "Review: This movie rocks!\n",
      "Sentiment: positive\n",
      "\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "review = \"This movie is not so great :(\"\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Sentiment: {get_prediction(review)}\")\n",
    "\n",
    "assert get_prediction(review) == \"negative\", \"The prediction should be negative\"\n",
    "\n",
    "\n",
    "review = \"This movie rocks!\"\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Sentiment: {get_prediction(review)}\")\n",
    "\n",
    "assert get_prediction(review) == \"positive\", \"The prediction should be positive\"\n",
    "print()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fc3e4f-7008-42f2-b7a0-088baaf42044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie is not so great :(\n",
      "Sentiment: Negative (84.98%)\n",
      "\tModel label: LABEL_0\n",
      "\tModel arg_index: 0\n",
      "\n",
      "Review: This movie rocks!\n",
      "Sentiment: Positive (97.81%)\n",
      "\tModel label: LABEL_1\n",
      "\tModel arg_index: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"This movie is not so great :(\"\n",
    "print(f\"Review: {review}\")\n",
    "use_model(review)\n",
    "print()\n",
    "review = \"This movie rocks!\"\n",
    "print(f\"Review: {review}\")\n",
    "use_model(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a05f54-1be3-4fcf-b9b4-914f73f7ad20",
   "metadata": {},
   "source": [
    "### Download a dataset from HuggingFace\n",
    "\n",
    "HuggingFace provides a number of datasets that can be used for a variety of tasks. In this exercise, we will use the `imdb` dataset and pass it to the model we instantiated in the previous exercise.\n",
    "\n",
    "Instructions:\n",
    "- Review the [loading a dataset](https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html) documentation\n",
    "- Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c3eb7b-5b87-49d7-a47a-0c28abc2e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea12b31-8082-43cd-a0fa-0ca3d994c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace <MASK> with the appropriate code\n",
    "# Load the test split of the imdb dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38a8530-cfb5-4c15-b922-703a47774585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV '\n",
      "         'are usually underfunded, under-appreciated and misunderstood. I '\n",
      "         'tried to like this, I really did, but it is to good TV sci-fi as '\n",
      "         'Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap '\n",
      "         \"cardboard sets, stilted dialogues, CG that doesn't match the \"\n",
      "         'background, and painfully one-dimensional characters cannot be '\n",
      "         \"overcome with a 'sci-fi' setting. (I'm sure there are those of you \"\n",
      "         \"out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      "         'clichéd and uninspiring.) While US viewers might like emotion and '\n",
      "         'character development, sci-fi is a genre that does not take itself '\n",
      "         'seriously (cf. Star Trek). It may treat important issues, yet not as '\n",
      "         \"a serious philosophy. It's really difficult to care about the \"\n",
      "         'characters here as they are not simply foolish, just missing a spark '\n",
      "         'of life. Their actions and reactions are wooden and predictable, '\n",
      "         \"often painful to watch. The makers of Earth KNOW it's rubbish as \"\n",
      "         'they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise '\n",
      "         \"people would not continue watching. Roddenberry's ashes must be \"\n",
      "         'turning in their orbit as this dull, cheap, poorly edited (watching '\n",
      "         'it without advert breaks really brings this home) trudging Trabant '\n",
      "         'of a show lumbers into space. Spoiler. So, kill off a main '\n",
      "         'character. And then bring him back as another actor. Jeeez! Dallas '\n",
      "         'all over again.'}\n",
      "\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "assert isinstance(dataset, Dataset), \"The dataset should be a Dataset object\"\n",
    "assert set(dataset.features.keys()) == {\n",
    "    \"label\",\n",
    "    \"text\",\n",
    "}, \"The dataset should have a label and a text feature\"\n",
    "\n",
    "# Show the first example\n",
    "pprint(dataset[0])\n",
    "print()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f3218-d6dd-4744-8462-08e61c10d83f",
   "metadata": {},
   "source": [
    "### Now let's use the pre-trained model!\n",
    "\n",
    "Let's make some predictions.\n",
    "\n",
    "Instructions:\n",
    "- Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6aca64-8a3c-4136-9cc3-f5ee61e7b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I got Monster Man in a box set of three films where I mainly wanted the other tw \n",
      "... ous, often gnarly splatter comedy that should endear itself to fans of the same.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n",
      "Review: Five minutes in, i started to feel how naff this was looking, you've got a compl \n",
      "... for anyone who likes their horror with several side orders of gore and attitude.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n",
      "Review: I caught this movie on the Sci-Fi channel recently. It actually turned out to be \n",
      "... e more than passable for the horror/slasher buff. Definitely worth checking out.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace <MASK> with the appropriate code\n",
    "\n",
    "# Get the last 3 reviews\n",
    "reviews = dataset[\"text\"][-3:]\n",
    "\n",
    "# Get the last 3 labels\n",
    "labels = dataset[\"label\"][-3:]\n",
    "\n",
    "# Check\n",
    "for review, label in zip(reviews, labels):\n",
    "    # Let's use your get_prediction function to get the sentiment\n",
    "    # of the review!\n",
    "    # prediction = <MASK>\n",
    "    # prediction = use_model(review)\n",
    "    prediction = get_prediction(review)\n",
    "\n",
    "    print(f\"Review: {review[:80]} \\n... {review[-80:]}\")\n",
    "    print(f'Label: {\"positive\" if label else \"negative\"}')\n",
    "    print(f\"Prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fab1ea-733c-4906-9c0d-da4195eb6155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "venv_ud_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
