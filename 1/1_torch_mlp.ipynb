{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c270923-4294-4b66-8315-698efcdaf7ef",
   "metadata": {},
   "source": [
    "# Making an MLP - Multilayer Perceptron - With Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bffa20b-60cb-441b-88b6-858675f89608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d4795d-4750-4e16-acca-4c31d4894da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size:int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(in_features=input_size, out_features=64)\n",
    "        self.output_layer = nn.Linear(64, 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        # Single dimension input to the softmax layer, so dim=0\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        # https://discuss.pytorch.org/t/implicit-dimension-choice-for-softmax-warning/12314/17\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     hidden_pass = self.hidden_layer(x)\n",
    "    #     activation_pass = self.activation(hidden_pass)\n",
    "    #     output_pass = self.output_layer(activation_pass)\n",
    "    #     return output_pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfec21a-f107-47ab-8e7c-a86d8abb1085",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super(MLP, self).__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden_layer = nn.Linear(in_features=input_size, out_features=64)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with a user determined number of inputs and 64 outputs and assigns it to `self.hidden_layer`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network (`mlp_net`) is created with `mlp_net.hidden_layer.weight` and `mlp_net.hidden_layer.bias`.\n",
    "\n",
    "```python\n",
    "self.output_layer = nn.Linear(64, 2)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 64 inputs and 2 outputs.\n",
    "\n",
    "```python\n",
    "self.activation = nn.ReLU()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden_layer(x)\n",
    "x = self.activation(x)\n",
    "x = self.output_layer(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation and reassigned to `x`. We can see that the input tensor goes through the hidden layer, then an activation function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `MLP` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b4b266-47ff-482d-8bb2-356c7be0635e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (softmax): Softmax(dim=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_net = MLP(input_size=10)\n",
    "print(mlp_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc13175d-519c-43ca-a789-d0dc0f7f8e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5523, 0.4477], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "mlp_net.forward(torch.rand(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "venv_ud_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
