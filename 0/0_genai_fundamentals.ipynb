{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Generative AI\n",
    "\n",
    "Generative AI is an exciting field of artificial intelligence that opens the door to creating new and original content, spanning from written text to stunning visuals and even computer-generated music. It showcases the innovative side of AI by going beyond simple analytical tasks to engage in creative processes.\n",
    "\n",
    "## Technical Terms Explained:\n",
    "* **Text Generation:** This involves making computers write text that makes sense and is relevant to the topic, akin to an automatic storyteller.\n",
    "\n",
    "* **Image Generation:** This allows computers to make new pictures or change existing ones, like a digital artist using a virtual paintbrush.\n",
    "\n",
    "* **Code Generation:** This is Gen AI for programming, where the computer helps write new code.\n",
    "\n",
    "* **Audio Generation:** Computers can also create sounds or music, a bit like a robot composer coming up with its own tunes.\n",
    "\n",
    "* **Chat GPT:** A language model developed by OpenAI that can generate responses similar to those a human would give in a conversation by predicting the next word in a sequence based on context.\n",
    "\n",
    "* **DALL¬∑E:** An AI program by OpenAI that produces images from textual descriptions, mimicking creativity in visual art.\n",
    "\n",
    "* **GitHub Copilot:** A coding assistant tool that suggests code snippets and completes code lines to help developers write more efficiently and with fewer errors.\n",
    "\n",
    "* **Contextual Suggestions:** Recommendations provided by AI tools, like Copilot, which are relevant to the current task or context within which a user is working.\n",
    "\n",
    "The applications of Generative AI span a gamut of exciting fields, broadening creativity and innovation in **content creation, product design, scientific inquiry, data enhancement, and personalized experiences**. The power of Generative AI lies in its ability to imagine and refine with speed, offering solutions and opening doors to future inventions.\n",
    "\n",
    "## Resource Links\n",
    "[Millions of New Materials Discovered with Deep Learning](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/) - Google DeepMind Research\n",
    "\n",
    "[Reinventing the wheel? ‚ÄúFelGAN‚Äù inspires new rim designs with AI](https://www.audi-mediacenter.com/en/press-releases/reinventing-the-wheel-felgan-inspires-new-rim-designs-with-ai-15097) - Audi\n",
    "\n",
    "[May the force of text data analysis be with you: Unleashing the power of generative AI for social psychology research](https://www.sciencedirect.com/science/article/pii/S2949882123000063) - Salah, Halbusi, and Abdelfattah (Computers in Human Behavior: Artificial Humans)\n",
    "\n",
    "For a deeper dive into synthetic data techniques, check out Udacity's course on [Small Datasets in Machine Learning](https://www.udacity.com/course/small-data--cd12528).\n",
    "\n",
    "The AI and machine learning timeline is a journey of technological breakthroughs starting with early advances like the perceptron in the 1950s, and moving through various challenges and innovations that have led to recent breakthroughs in generative AI. This timeline shows us the perseverance and ingenuity involved in the evolution of AI, highlighting how each decade built upon the last to reach today's exciting capabilities.\n",
    "\n",
    "<img src=\"img/img_00.png\">\n",
    "\n",
    "## Technical terms explained:\n",
    "\n",
    "* **Perceptron:** An early type of neural network component that can decide whether or not an input, represented by numerical values, belongs to a specific class.\n",
    "\n",
    "* **Neural Networks:** Computer systems modeled after the human brain that can learn from data by adjusting the connections between artificial neurons.\n",
    "\n",
    "* **Back Propagation:** A method used in artificial neural networks to improve the model by adjusting the weights by calculating the gradient of the loss function.\n",
    "\n",
    "* **Statistical Machine Learning:** A method of data analysis that automates analytical model building using algorithms that learn from data without being explicitly programmed.\n",
    "\n",
    "* **Deep Learning:** A subset of machine learning composed of algorithms that permit software to train itself to perform tasks by exposing multilayered neural networks to vast amounts of data.\n",
    "\n",
    "* **Generative Adversarial Networks (GANs):** A class of machine learning models where two networks, a generator and a discriminator, are trained simultaneously in a zero-sum game framework.\n",
    "\n",
    "* **Transformer:** A type of deep learning model that handles sequential data and is particularly noted for its high performance in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Familiarize Yourself with a Commercial Large Language Model (LLM)\n",
    "\n",
    "Large Language Models (LLMs) are revolutionizing the AI domain with their ability to parse and generate human language. From content creation to personal assistants, their applications are vast and varied.\n",
    "\n",
    "In this exercise, you will familiarize yourself with a commercial LLM. You will explore its capabilities and think about how you could use it in your own work or personal projects.\n",
    "\n",
    "First, choose one of many commercial LLMs with a chat interface. Some include:\n",
    "\n",
    "* [ChatGPT](https://chat.openai.com/) from OpenAI\n",
    "* [Bing](https://www.bing.com/) from Microsoft\n",
    "* [Gemini](https://gemini.google.com/) from Google\n",
    "\n",
    "Second, create an account with the LLM provider if you haven't already. You may need to provide a credit card number, but you can use the free trial period to explore the LLM's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Generative AI Models\n",
    "\n",
    "The exciting world of training generative AI models is about teaching computers to create new content, like text or images, by learning from huge datasets. This training helps AI to understand and recreate the complex patterns found in human language and visual arts. The process is intricate but immensely rewarding, leading to AI that can generate amazingly realistic outputs.\n",
    "\n",
    "## Technical Terms Explained:\n",
    "\n",
    "* **Large Language Models (LLMs)**: These are AI models specifically designed to understand and generate human language by being trained on a vast amount of text data.\n",
    "\n",
    "* **Variational Autoencoders (VAEs)**: A type of AI model that can be used to create new images. It has two main parts: the encoder reduces data to a simpler form, and the decoder expands it back to generate new content.\n",
    "\n",
    "* **Latent Space**: A compressed representation of data that the autoencoder creates in a simpler, smaller form, which captures the most important features needed to reconstruct or generate new data.\n",
    "\n",
    "* **Parameters**: Parameters are the variables that the model learns during training. They are internal to the model and are adjusted through the learning process. In the context of neural networks, parameters typically include weights and biases.\n",
    "\n",
    "* **Weights**: Weights are coefficients for the input data. They are used in calculations to determine the importance or influence of input variables on the model's output. In a neural network, each connection between neurons has an associated weight.\n",
    "\n",
    "* **Biases**: Biases are additional constants attached to neurons and are added to the weighted input before the activation function is applied. Biases ensure that even when all the inputs are zero, there can still be a non-zero output.\n",
    "\n",
    "* **Hyperparameters**: Hyperparameters, unlike parameters, are not learned from the data. They are more like settings or configurations for the learning process. They are set prior to the training process and remain constant during training. They are external to the model and are used to control the learning process.\n",
    "\n",
    "Generation algorithms are incredible tools that allow AI to create text and images that seem amazingly human-like. By understanding and applying these smart algorithms, AI can generate new content by building upon what it knows, just like filling in missing puzzle pieces.\n",
    "\n",
    "## Technical Terms Explained:\n",
    "\n",
    "* **Autoregressive text generation**: Autoregressive text generation is like a game where the computer guesses the next word in a sentence based on the words that came before it. It keeps doing this to make full sentences.\n",
    "\n",
    "* **Latent space decoding**: Imagine if you had a map of all the possible images you could create, with each point on the map being a different image. Latent space decoding is like picking a point on that map and bringing the image at that point to life.\n",
    "\n",
    "* **Diffusion models**: Diffusion models start with a picture that's full of random dots like TV static, and then they slowly clean it up, adding bits of the actual picture until it looks just like a real photo or painting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Generating one token at a time\n",
    "\n",
    "Generative AI models can create text by predicting one token at a time, with each token representing a piece of a word or sometimes a whole word. This process involves using probabilities to determine the most likely next piece of text, which helps the model construct coherent sentences.\n",
    "\n",
    "In this exercise, we will get to understand how an LLM generates text--one token at a time, using the previous tokens to predict the following ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load a tokenizer and a model\n",
    "\n",
    "First we load a tokenizer and a model from HuggingFace's transformers library. A tokenizer is a function that splits a string into a list of numbers that the model can understand.\n",
    "\n",
    "In this exercise, all the code will be written for you. All you need to do is follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# To load a pretrained model and a tokenizer using HuggingFace, we only need two lines of code!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# We create a partial sentence and tokenize it.\n",
    "text = \"Udacity is the best place to learn about generative\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Show the tokens as numbers, i.e. \"input_ids\"\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Examine the tokenization\n",
    "\n",
    "Let's explore what these tokens mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(52)</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(67)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(4355)</td>\n",
       "      <td>acity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(318)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(1266)</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(1295)</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(284)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(2193)</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(546)</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(1152)</td>\n",
       "      <td>gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(876)</td>\n",
       "      <td>ative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   token\n",
       "0     tensor(52)       U\n",
       "1     tensor(67)       d\n",
       "2   tensor(4355)   acity\n",
       "3    tensor(318)      is\n",
       "4    tensor(262)     the\n",
       "5   tensor(1266)    best\n",
       "6   tensor(1295)   place\n",
       "7    tensor(284)      to\n",
       "8   tensor(2193)   learn\n",
       "9    tensor(546)   about\n",
       "10  tensor(1152)   gener\n",
       "11   tensor(876)   ative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how the sentence is tokenized\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def show_tokenization(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
    "        columns=[\"id\", \"token\"],\n",
    "    )\n",
    "\n",
    "\n",
    "show_tokenization(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "\n",
    "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
    "\n",
    "## Step 2. Calculate the probability of the next token\n",
    "\n",
    "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>8300</td>\n",
       "      <td>programming</td>\n",
       "      <td>0.157589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>4673</td>\n",
       "      <td>learning</td>\n",
       "      <td>0.148414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>4981</td>\n",
       "      <td>models</td>\n",
       "      <td>0.048506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>17219</td>\n",
       "      <td>biology</td>\n",
       "      <td>0.046481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16113</th>\n",
       "      <td>16113</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>0.027796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id         token         p\n",
       "8300    8300   programming  0.157589\n",
       "4673    4673      learning  0.148414\n",
       "4981    4981        models  0.048506\n",
       "17219  17219       biology  0.046481\n",
       "16113  16113    algorithms  0.027796"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the probabilities for the next token for all possible choices. We show the\n",
    "# top 5 choices and the corresponding words or subwords for these tokens.\n",
    "\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "\n",
    "def show_next_token_choices(probabilities, top_n=5):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns=[\"id\", \"token\", \"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! The model thinks that the most likely next word is \"programming\", followed up closely by \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 8300\n",
      "Next token:  programming\n"
     ]
    }
   ],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Udacity is the best place to learn about generative programming'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We append the most likely token to the text.\n",
    "text = text + tokenizer.decode(8300)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Generate some more tokens\n",
    "\n",
    "The following cell will take `text`, show the most probable tokens to follow, and append the most likely token to text. Run the cell over and over to see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udacity is the best place to learn about generative programming\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Next token probabilities:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>0.352227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>,</td>\n",
       "      <td>0.135983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>and</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>in</td>\n",
       "      <td>0.069529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>8950</td>\n",
       "      <td>languages</td>\n",
       "      <td>0.058288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       token         p\n",
       "13      13           .  0.352227\n",
       "11      11           ,  0.135983\n",
       "290    290         and  0.109372\n",
       "287    287          in  0.069529\n",
       "8950  8950   languages  0.058288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Press ctrl + enter to run this cell again and again to see how the text is generated.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Show the text\n",
    "print(text)\n",
    "\n",
    "# Convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate the probabilities for the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token id and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Once upon a time, generative models of the human brain were used to study the neural correlates of cognitive function. In the present study, we used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Start with some text and tokenize it\n",
    "text = \"Once upon a time, generative models\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the `generate` method to generate lots of text\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's interesting...\n",
    "\n",
    "You'll notice that GPT-2 is not nearly as sophisticated as later models like GPT-4, which you may have experience using. It often repeats itself and doesn't always make much sense. But it's still pretty impressive that it can generate text that looks like English.\n",
    "\n",
    "## Congrats for completing the exercise! üéâ\n",
    "\n",
    "Give yourself a hand. And please take a break if you need to. We'll be here when you're refreshed and ready to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Generative AI Architectures\n",
    "\n",
    "There are various Generative AI Architectures for creating new content by mimicking patterns. These architectures, like GANs, RNNs, and Transformers, excel at producing novel images, text, and sounds by understanding and repurposing what they've learned. They enable us to push the boundaries of creativity and innovation, opening up a world of new possibilities.\n",
    "\n",
    "## Technical Terms Explained:\n",
    "\n",
    "* **Generative Adversarial Networks (GANs)**: A system where two neural networks, one to generate data and one to judge it, work against each other. This competition helps improve the quality of the generated results.\n",
    "\n",
    "* **Recurrent Neural Networks (RNNs)**: A network that's really good at handling sequences, like sentences or melodies, because it processes one piece at a time and remembers what it saw before.\n",
    "\n",
    "* **Transformer-based models**: A more advanced type that looks at whole sequences at once, not one piece at a time, making it faster and smarter at tasks like writing sentences or translating languages.\n",
    "\n",
    "* **Sequential Data**: Data that is connected in a specific order, like words in a sentence or steps in a dance routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challengs in Generative AI\n",
    "\n",
    "Generative AI presents exciting opportunities but also involves some challenges that need thoughtful consideration. While these technologies show great promise in creativity and efficiency, it's important to address issues such as misleading information, job displacement, the originality of art, and environmental impacts.\n",
    "\n",
    "## Technical Terms Explained:\n",
    "\n",
    "* **Deepfakes**: Highly realistic fake videos or images created by AI, which can make it seem like people are saying or doing things they never actually did.\n",
    "\n",
    "* **Automation**: The use of technology to perform tasks without human intervention, which can increase efficiency but also may replace jobs done by people.\n",
    "\n",
    "* **Copyright Issues**: Legal problems that arise when someone uses work without permission, potentially impacting the original creator's rights.\n",
    "\n",
    "* **Carbon Footprint**: The total amount of greenhouse gases produced directly or indirectly by activities or entities, like running large-scale AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud_gen",
   "language": "python",
   "name": "venv_ud_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
